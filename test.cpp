// Î©ÄÌã∞ ÎîîÎ∞îÏù¥Ïä§ (MULTI, AUTO, HETERO) ÏÑ§Ï†ïÎ≥Ñ LLM ÌååÏù¥ÌîÑÎùºÏù∏ ÏòàÏ†ú
// OpenVINO GenAIÏóêÏÑú Îã§ÏñëÌïú ÎîîÎ∞îÏù¥Ïä§ Ï°∞Ìï©ÏúºÎ°ú Ïã§Ìñâ Í∞ÄÎä•Ìïú Íµ¨Ï°∞

#include "src/llm/pipeline_stateful.hpp"
#include "openvino/core/core.hpp"
#include "openvino/runtime/core.hpp"

using namespace ov;

constexpr size_t MAX_PROMPT_LEN = 32;
constexpr size_t MIN_RESPONSE_LEN = 32;

AnyMap configure_properties_for_device(const std::string& device) {
    AnyMap properties;
    if (device.find("NPU") != std::string::npos) {
        properties["STATIC_PIPELINE"] = true;
        properties["MAX_PROMPT_LEN"] = MAX_PROMPT_LEN;
        properties["MIN_RESPONSE_LEN"] = MIN_RESPONSE_LEN;
        properties["NUM_STREAMS"] = 1;
        properties["CACHE_DIR"] = "./ov_cache";
        properties["PERFORMANCE_HINT"] = "LATENCY";
    } else if (device.find("MULTI") != std::string::npos) {
        properties["MULTI_DEVICE_PRIORITIES"] = "NPU,CPU";
        properties["CACHE_DIR"] = "./ov_cache";
    } else if (device.find("AUTO") != std::string::npos) {
        properties["CACHE_DIR"] = "./ov_cache";
    } else if (device.find("HETERO") != std::string::npos) {
        properties["CACHE_DIR"] = "./ov_cache";
    }
    return properties;
}

std::shared_ptr<Model> load_and_reshape_model(const std::filesystem::path& model_path, const std::string& device) {
    Core core;
    auto model = core.read_model(model_path.string());
    if (device.find("NPU") != std::string::npos || device.find("MULTI") != std::string::npos || device.find("AUTO") != std::string::npos) {
        model->reshape({
            {"input_ids", PartialShape{1, MAX_PROMPT_LEN}},
            {"attention_mask", PartialShape{1, MAX_PROMPT_LEN}}
        });
    }
    return model;
}

StatefulLLMPipeline build_pipeline(const std::filesystem::path& model_path,
                                   const Tokenizer& tokenizer,
                                   const std::string& device) {
    auto properties = configure_properties_for_device(device);
    auto model = load_and_reshape_model(model_path / "openvino_model.xml", device);

    GenerationConfig gen_config;
    gen_config.max_length = MAX_PROMPT_LEN + MIN_RESPONSE_LEN;
    gen_config.eos_token_id = tokenizer.get_eos_token_id();
    gen_config.num_return_sequences = 1;
    gen_config.do_sample = false;

    if (device.find("NPU") != std::string::npos ||
        device.find("MULTI") != std::string::npos ||
        device.find("AUTO") != std::string::npos) {
        gen_config.set_beam_search(false);
        gen_config.set_multinomial(false);
        gen_config.set_greedy(true);
    }

    return StatefulLLMPipeline(model, tokenizer, device, properties, gen_config);
}

int main() {
    std::string model_dir = "./Llama-2-7B-Chat-FP16";
    std::vector<std::string> device_list = {
        "MULTI:NPU,CPU",
        "AUTO",
        "HETERO:NPU,CPU"
    };

    Tokenizer tokenizer(model_dir);

    for (const auto& device : device_list) {
        std::cout << "\n[Running on device: " << device << "]\n" << std::endl;

        auto pipeline = build_pipeline(model_dir, tokenizer, device);
        std::string prompt = "What is the capital of France?";
        auto result = pipeline.generate(prompt);

        for (const auto& output : result.texts) {
            std::cout << "[Answer]: " << output << std::endl;
        }
    }

    return 0;
}

/*
üìä ÏÑ±Îä• ÎπÑÍµê Í∞ÄÏù¥Îìú (ÏòàÏÉÅ Í∏∞Ï§Ä)
--------------------------------------------------
| ÎîîÎ∞îÏù¥Ïä§ Î™®Îìú     | ÏÑ§Î™Ö                         | Ïû•Ï†ê            | Îã®Ï†ê              |
|------------------|------------------------------|-----------------|-------------------|
| MULTI:NPU,CPU    | NPU Ïö∞ÏÑ†, Ïã§Ìå® Ïãú CPU ÏÇ¨Ïö©    | ÏïàÏ†ïÏÑ± + ÏÑ±Îä• Í∞ÄÎä•ÏÑ± | ÏÑ§Ï†ï ÌïÑÏöî          |
| AUTO             | ÏûêÎèô ÏÑ†ÌÉù (ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïû•Ïπò) | ÏÑ§Ï†ï Í∞ÑÌé∏       | ÏÑ†ÌÉù Ï†úÏñ¥ Ïñ¥Î†§ÏõÄ     |
| HETERO:NPU,CPU   | ÎÖ∏Îìú Îã®ÏúÑ ÎîîÎ∞îÏù¥Ïä§ Î∂ÑÌï† Ïã§Ìñâ | ÏÑ∏Î∞ÄÌïú Ï†úÏñ¥ Í∞ÄÎä• | Î≥µÏû°Ìï® + ÏòàÏô∏ Í∞ÄÎä•ÏÑ± |
--------------------------------------------------
*/
