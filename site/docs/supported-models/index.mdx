import ImageGenerationModelsTable from './_components/image-generation-models-table';
import LLMModelsTable from './_components/llm-models-table';
import VLMModelsTable from './_components/vlm-models-table';
import WhisperModelsTable from './_components/whisper-models-table';
import TextEmbeddingsModelsTable from './_components/text-embeddings-models-table';
import SpeechGenerationModelsTable from './_components/speech-generation-models-table';
import TextRerankModelsTable from './_components/text-rerank-models-table';


# Supported Models

:::info

Other models with similar architectures may also work successfully even if not explicitly validated.
Consider testing any unlisted models to verify compatibility with your specific use case.

:::

## Large Language Models (LLMs)

<LLMModelsTable />

:::info

LoRA adapters are supported.

:::

::::info

The pipeline can work with other similar topologies produced by `optimum-intel` with the same model signature.
The model is required to have the following inputs after the conversion:

1. `input_ids` contains the tokens.
2. `attention_mask` is filled with `1`.
3. `beam_idx` selects beams.
4. `position_ids` (optional) encodes a position of currently generating token in the sequence and a single `logits` output.

:::note

Models should belong to the same family and have the same tokenizers.

:::

::::

## Image Generation Models

<ImageGenerationModelsTable />

## Visual Language Models (VLMs)

<VLMModelsTable />

:::warning VLM Models Notes
#### InternVL2 {#internvl2-notes}

To convert InternVL2 models, `timm` and `einops` are required:

```bash
pip install timm einops
```
#### MiniCPMO {#minicpm-o-notes}

1. `openbmb/MiniCPM-o-2_6` doesn't support transformers>=4.52 which is required for `optimum-cli` export.
2. `--task image-text-to-text` is required for `optimum-cli export openvino --trust-remote-code` because `image-text-to-text` isn't `MiniCPM-o-2_6`'s native task.

#### phi3_v {#phi3_v-notes}

Models' configs aren't consistent. It's required to override the default `eos_token_id` with the one from a tokenizer:
```python
generation_config.set_eos_token_id(pipe.get_tokenizer().get_eos_token_id())
```
#### phi4mm {#phi4mm-notes}

Apply https://huggingface.co/microsoft/Phi-4-multimodal-instruct/discussions/78/files to fix the model export for transformers>=4.50
:::

## Speech Recognition Models (Whisper-based)

<WhisperModelsTable />

## Text Embeddings Models

<TextEmbeddingsModelsTable />

## Speech Generation Models

<SpeechGenerationModelsTable />

## Text Rerank Models

<TextRerankModelsTable />

:::info
LoRA adapters are not supported.
:::

:::info

Some models may require access request submission on the Hugging Face page to be downloaded.

If https://huggingface.co/ is down, the conversion step won't be able to download the models.

:::
