import BasicGenerationConfiguration from '@site/docs/use-cases/_shared/_basic_generation_configuration.mdx';
import ChatScenario from '@site/docs/use-cases/_shared/_chat_scenario.mdx';
import GenerationConfigurationWorkflow from '@site/docs/use-cases/_shared/_generation_configuration_workflow.mdx';
import Streaming from '@site/docs/use-cases/_shared/_streaming.mdx';

## Additional Usage Options

:::tip
Check out [Python](https://github.com/openvinotoolkit/openvino.genai/tree/master/samples/python/visual_language_chat) and [C++](https://github.com/openvinotoolkit/openvino.genai/tree/master/samples/cpp/visual_language_chat) visual language chat samples.
:::

### Use Image Tags in Prompt

The prompt can contain `<ov_genai_image_i>` with `i` replaced with an actual zero based index to refer to an image. Reference to images used in previous prompts isn't implemented. A model's native image tag can be used instead of `<ov_genai_image_i>`. These tags are:
1. InternVL2: `<image>\n`
2. llava-1.5-7b-hf: `<image>`
3. LLaVA-NeXT: `<image>`
4. MiniCPM-V-2_6: `(<image>./</image>)\n`
5. Phi-3-vision: `<|image_i|>\n` - the index starts with one
6. Phi-4-multimodal-instruct: `<|image_i|>\n` - the index starts with one
7. Qwen2-VL: `<|vision_start|><|image_pad|><|vision_end|>`
8. Qwen2.5-VL: `<|vision_start|><|image_pad|><|vision_end|>`
9. gemma-3-4b-it: `<start_of_image>`

If the prompt doesn't contain image tags, but images are provided, the tags are prepended to the prompt.

### Use Different Generation Parameters

Similar to [text generation](/docs/use-cases/text-generation/#use-different-generation-parameters), VLM pipelines support various generation parameters to control the text output.

<GenerationConfigurationWorkflow />

<BasicGenerationConfiguration>
  <LanguageTabs>
      <TabItemPython>
          ```python
          import openvino_genai as ov_genai

          pipe = ov_genai.VLMPipeline(model_path, "CPU")

          # Get default configuration
          config = pipe.get_generation_config()

          # Modify parameters
          config.max_new_tokens = 100
          config.temperature = 0.7
          config.top_k = 50
          config.top_p = 0.9
          config.repetition_penalty = 1.2

          # Generate text with custom configuration
          output = pipe.generate(prompt, images, config)
          ```
      </TabItemPython>
      <TabItemCpp>
          ```cpp
          int main() {
              ov::genai::VLMPipeline pipe(model_path, "CPU");

              // Get default configuration
              auto config = pipe.get_generation_config();

              // Modify parameters
              config.max_new_tokens = 100;
              config.temperature = 0.7f;
              config.top_k = 50;
              config.top_p = 0.9f;
              config.repetition_penalty = 1.2f;

              // Generate text with custom configuration
              auto output = pipe.generate(prompt, images, config);
          }
          ```
      </TabItemCpp>
  </LanguageTabs>
</BasicGenerationConfiguration>

<ChatScenario />

<Streaming />
