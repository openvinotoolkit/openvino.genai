### Using GenAI in Chat Scenario

For chat applications, OpenVINO GenAI provides special optimizations to maintain conversation context and improve performance using KV-cache.

:::tip
Use `start_chat()` and `finish_chat()` to properly manage the chat session's KV-cache. This improves performance by reusing context between messages.
:::

A simple chat example (with grouped beam search decoding):

<LanguageTabs>
    <TabItemPython>
        ```python showLineNumbers
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(model_path, 'CPU')

        config = {'max_new_tokens': 100, 'num_beam_groups': 3, 'num_beams': 15, 'diversity_penalty': 1.5}
        pipe.set_generation_config(config)

        # highlight-next-line
        pipe.start_chat()
        while True:
            try:
                prompt = input('question:\n')
            except EOFError:
                break
            answer = pipe.generate(prompt)
            print('answer:\n')
            print(answer)
            print('\n----------\n')
        # highlight-next-line
        pipe.finish_chat()
        ```
    </TabItemPython>
    <TabItemCpp>
        ```cpp showLineNumbers
        #include "openvino/genai/llm_pipeline.hpp"
        #include <iostream>

        int main(int argc, char* argv[]) {
            std::string prompt;

            std::string model_path = argv[1];
            ov::genai::LLMPipeline pipe(model_path, "CPU");

            ov::genai::GenerationConfig config;
            config.max_new_tokens = 100;
            config.num_beam_groups = 3;
            config.num_beams = 15;
            config.diversity_penalty = 1.0f;

            // highlight-next-line
            pipe.start_chat();
            std::cout << "question:\n";
            while (std::getline(std::cin, prompt)) {
                std::cout << "answer:\n";
                auto answer = pipe.generate(prompt, config);
                std::cout << answer << std::endl;
                std::cout << "\n----------\n"
                    "question:\n";
            }
            // highlight-next-line
            pipe.finish_chat();
        }
        ```
    </TabItemCpp>
</LanguageTabs>

#### Streaming the Output

For more interactive UIs during generation, you can stream output tokens.

##### Streaming Function

In this example, a function outputs words to the console immediately upon generation:

<LanguageTabs>
    <TabItemPython>
        ```python showLineNumbers
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(model_path, "CPU")

        # highlight-start
        # Create a streamer function
        def streamer(subword):
            print(subword, end='', flush=True)
            # Return flag corresponds whether generation should be stopped.
            return ov_genai.StreamingStatus.RUNNING
        # highlight-end

        # highlight-next-line
        pipe.start_chat()
        while True:
            try:
                prompt = input('question:\n')
            except EOFError:
                break
            # highlight-next-line
            pipe.generate(prompt, streamer=streamer, max_new_tokens=100)
            print('\n----------\n')
        # highlight-next-line
        pipe.finish_chat()
        ```
    </TabItemPython>
    <TabItemCpp>
        ```cpp showLineNumbers
        #include "openvino/genai/llm_pipeline.hpp"
        #include <iostream>

        int main(int argc, char* argv[]) {
            std::string prompt;

            std::string model_path = argv[1];
            ov::genai::LLMPipeline pipe(model_path, "CPU");

            // highlight-start
            // Create a streamer function
            auto streamer = [](std::string word) {
                std::cout << word << std::flush;
                // Return flag corresponds whether generation should be stopped.
                return ov::genai::StreamingStatus::RUNNING;
            };
            // highlight-end

            // highlight-next-line
            pipe.start_chat();
            std::cout << "question:\n";
            while (std::getline(std::cin, prompt)) {
                // highlight-next-line
                pipe.generate(prompt, ov::genai::streamer(streamer), ov::genai::max_new_tokens(100));
                std::cout << "\n----------\n"
                    "question:\n";
            }
            // highlight-next-line
            pipe.finish_chat();
        }
        ```
    </TabItemCpp>
</LanguageTabs>

:::info
For more information, refer to the [chat sample](https://github.com/openvinotoolkit/openvino.genai/tree/master/samples/python/chat_sample/).
:::

##### Custom Streamer Class

You can also create your custom streamer for more sophisticated processing:

<LanguageTabs>
    <TabItemPython>
        ```python showLineNumbers
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(model_path, "CPU")

        # highlight-start
        # Create custom streamer class
        class CustomStreamer(ov_genai.StreamerBase):
            def __init__(self):
                super().__init__()
                # Initialization logic.

            def write(self, token_id) -> bool:
                # Custom decoding/tokens processing logic.

                # Return flag corresponds whether generation should be stopped.
                return ov_genai.StreamingStatus.RUNNING

            def end(self):
                # Custom finalization logic.
                pass
        # highlight-end

        # highlight-next-line
        pipe.start_chat()
        while True:
            try:
                prompt = input('question:\n')
            except EOFError:
                break
            # highlight-next-line
            pipe.generate(prompt, streamer=CustomStreamer(), max_new_tokens=100)
            print('\n----------\n')
        # highlight-next-line
        pipe.finish_chat()
        ```
    </TabItemPython>
    <TabItemCpp>
        ```cpp showLineNumbers
        #include "openvino/genai/streamer_base.hpp"
        #include "openvino/genai/llm_pipeline.hpp"
        #include <iostream>

        // highlight-start
        // Create custom streamer class
        class CustomStreamer: public ov::genai::StreamerBase {
        public:
            bool write(int64_t token) {
                // Custom decoding/tokens processing logic.

                // Return flag corresponds whether generation should be stopped.
                return ov::genai::StreamingStatus::RUNNING;
            };

            void end() {
                // Custom finalization logic.
            };
        };
        // highlight-end

        int main(int argc, char* argv[]) {
            std::string prompt;
            // highlight-next-line
            CustomStreamer custom_streamer;

            std::string model_path = argv[1];
            ov::genai::LLMPipeline pipe(model_path, "CPU");

            // highlight-next-line
            pipe.start_chat();
            std::cout << "question:\n";
            while (std::getline(std::cin, prompt)) {
                // highlight-next-line
                pipe.generate(prompt, ov::genai::streamer(custom_streamer), ov::genai::max_new_tokens(100));
                std::cout << "\n----------\n"
                    "question:\n";
            }
            // highlight-next-line
            pipe.finish_chat();
        }
        ```
    </TabItemCpp>
</LanguageTabs>

:::info
For fully implemented iterable CustomStreamer refer to [multinomial_causal_lm](https://github.com/openvinotoolkit/openvino.genai/tree/releases/2024/5/samples/python/multinomial_causal_lm) sample.
:::
