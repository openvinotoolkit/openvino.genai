# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0


import pytest
import openvino_genai as ov_genai
from utils.hugging_face import download_and_convert_model, run_hugging_face
from utils.comparation import compare_generation_results
from utils.ov_genai_pipelines import create_ov_pipeline, PipelineType, convert_decoded_results_to_generation_result

eagle_models_and_input = [
    ("Qwen/Qwen3-1.7B", "AngelSlim/Qwen3-1.7B_eagle3", "Why is the Sun yellow?")]

devices = [
    ('CPU', 'CPU'),
    ('GPU', 'GPU')
]
@pytest.mark.parametrize("main_model,draft_model,prompt", eagle_models_and_input)
@pytest.mark.parametrize("main_device,draft_device", devices)
@pytest.mark.precommit
@pytest.mark.skip(reason="CVS-174959 enable model conversion for eagle3 and enable the test")
def test_eagle3_sd_string_inputs(main_model, main_device, draft_model, draft_device, prompt):
    # Download and convert model:
    main_opt_model, main_hf_tokenizer, main_model_path = download_and_convert_model(main_model)
    print("finised")
    __, __, draft_model_path = download_and_convert_model(draft_model)

    # Create OpenVINO GenAI pipeline:

    ov_pipe = create_ov_pipeline(main_model_path, pipeline_type = PipelineType.SPECULATIVE_DECODING, draft_model_path = draft_model_path)

    # Run reference HF model:
    ov_generation_config = ov_genai.GenerationConfig(max_new_tokens=20)
    main_hf_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    ref_gen_results = run_hugging_face(main_opt_model, main_hf_tokenizer, [prompt], ov_generation_config)  

    # Run OpenVINO GenAI pipeline:
    ov_decoded_results = ov_pipe.generate([prompt], ov_generation_config)
    ov_gen_results = convert_decoded_results_to_generation_result(ov_decoded_results, 1, 1, False)

    del ov_pipe

    # Compare results:
    compare_generation_results([prompt], ref_gen_results, ov_gen_results, ov_generation_config)

@pytest.mark.parametrize("main_model,draft_model,prompt", eagle_models_and_input)
@pytest.mark.parametrize("main_device,draft_device", devices)
@pytest.mark.precommit
@pytest.mark.skip(reason="CVS-174959 enable model conversion for eagle3 and enable the test")
def test_eagle3_sd_extended_perf_metrics(main_model, main_device, draft_model, draft_device, prompt):
    import time
    extended_perf_metrics = None
    start_time = time.perf_counter()
    generation_config = ov_genai.GenerationConfig(do_sample=False, max_new_tokens=20, ignore_eos=True, num_assistant_tokens=5)
    # Download and convert model:
    main_opt_model, main_hf_tokenizer, main_model_path = download_and_convert_model(main_model)
    __, __, draft_model_path = download_and_convert_model(draft_model)

    # Create OpenVINO GenAI pipeline:
    ov_pipe = create_ov_pipeline(main_model_path, pipeline_type = PipelineType.SPECULATIVE_DECODING, draft_model_path = draft_model_path)
    extended_perf_metrics = ov_pipe.generate([prompt], generation_config).extended_perf_metrics
    total_time = (time.perf_counter() - start_time) * 1000

    assert not extended_perf_metrics is None
    assert not extended_perf_metrics.main_model_metrics is None
    assert not extended_perf_metrics.draft_model_metrics is None

    assert extended_perf_metrics.get_num_accepted_tokens() > 0

    num_generated_tokens_main = extended_perf_metrics.main_model_metrics.get_num_generated_tokens()
    assert num_generated_tokens_main > 0 and num_generated_tokens_main <= generation_config.max_new_tokens

    # max num_generated_tokens for draft model will be reached if it will generate num_assistant_tokens at each step
    # plus fist token, which was generated by main model
    num_generated_tokens_draft = extended_perf_metrics.draft_model_metrics.get_num_generated_tokens()
    assert num_generated_tokens_draft > 0 and num_generated_tokens_draft < ((generation_config.max_new_tokens - 1) * generation_config.num_assistant_tokens + 1)

    total_iteration_number_main = len(extended_perf_metrics.main_model_metrics.raw_metrics.m_durations)
    assert total_iteration_number_main > 0 and total_iteration_number_main <= generation_config.max_new_tokens

    total_iteration_number_draft = len(extended_perf_metrics.draft_model_metrics.raw_metrics.m_durations)
    assert total_iteration_number_draft > 0 and total_iteration_number_draft < ((generation_config.max_new_tokens - 1) * generation_config.num_assistant_tokens + 1)

    for model_metrics in [extended_perf_metrics.main_model_metrics, extended_perf_metrics.draft_model_metrics]:
        mean_ttst, std_ttst = model_metrics.get_ttst()
        assert (mean_ttst, std_ttst) == (model_metrics.get_ttst().mean, model_metrics.get_ttst().std)
        assert mean_ttst > 0 and mean_ttst < model_metrics.get_ttft().mean
        assert std_ttst == 0

        mean_latency, std_latency = model_metrics.get_latency()
        assert (mean_latency, std_latency) == (model_metrics.get_latency().mean, model_metrics.get_latency().std)
        assert mean_latency > 0 and mean_latency < 1000.0

        mean_gen_duration, std_gen_duration = model_metrics.get_generate_duration()
        assert (mean_gen_duration, std_gen_duration) == (model_metrics.get_generate_duration().mean, model_metrics.get_generate_duration().std)
        assert mean_gen_duration > 0 and mean_gen_duration < total_time
        assert std_gen_duration == 0